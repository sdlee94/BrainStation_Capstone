# My BrainStation Capstone Project: Deep Learning for DeepFake Detection
---

### What are DeepFakes?

Take a look at this GIF. Which one do you think is the original clip?  

<p align="center">
  <img src="https://github.com/sdlee94/BrainStation_Capstone/blob/master/deepfake.gif"/>
  <br/>
  <a href="https://tenor.com/view/deep-fakes-ctrl-shift-face-smiles-shrugs-gif-14641388">Source</a>
</p>  

Difficult, right? Especially if you don't know what movie this is from. Did you know this was generated by a computer? Well if you aren't aware, this is a **DeepFake**, an altered video produced by **Artificial Intelligence (AI)**. And it's not just limited to face swapping, any aspect of digital media is subject to 'DeepFaking' - a person's mouth movements can be adjusted to match any audio sample, for instance. It also takes a fraction of the time and cost to make DeepFakes than if a person were to produce the same results with CGI. This technology brings major implications to society; from spreading false information to fabricating evidence in court to sabotaging politics, it's not hard to imagine the many dangers of DeepFakes if left unchecked. Imagine if DeepFakes were to come out today of health officials and political figures saying that the Covid-19 pandemic is over, everyone can go out and socialize - the consequences would be disastrous!

### DeepFake Detection Challenge  

In an effort to curb the emerging threat of DeepFakes, a [**Kaggle competition**](https://www.kaggle.com/c/deepfake-detection-challenge/overview) was built in collaboration between Amazon, Facebook, Microsoft and Partnership on AI to invite enthusiasts of all backgrounds to compete for the best performing DeepFake detection model. As someone who loves challenging problems and has genuine concerns about DeepFakes, I chose to tackle this challenge for my capstone project. Over the course of ~7 weeks I performed an end-to-end Data Science workflow in which I obtained data, processed it and trained several deep learning models to differentiate between real and fake videos. To date, my best model achieved **96% precision** (of all predicted fakes, 96% were indeed fake) and **83% specificity** (able to correctly identify 83% of real videos) using just a single frame per video. Below details the tools used for this project and an overview of the steps I took to get there!

## Resources
---

### Tools Used
- Bash
- Python
- Github
- Google Colab

#### Python Packages:
- **Data Science**: numpy, pandas
- **Plotting**: matplotlib
- **Machine Learning**: tensorflow version 2.1, keras
- **Other**: jupyter, os, imageio, pickle, h5py

### Dataset

The competition dataset consisted of close to 500 GB of videos, each with a length of 10 seconds at 30 frames per second (FPS). Due to computational and time limitations, I opted to use a pre-processed dataset which consisted of 160x160 resolution images of extracted faces from the original videos. Credits goes to *Hieu Phung* for generating this [**dataset**](https://www.kaggle.com/c/deepfake-detection-challenge/discussion/128954) - information about the pre-processing workflow can be found [**here**](https://www.kaggle.com/phunghieu/deepfake-detection-face-extractor). I also downloaded a subsample of the entire dataset, consisting of 10,420 videos (extracted frames).  

The dataset was split into several parts, with a zip file containing a variable number of folders with each containing a set of images pertaining to a unique video. Each zip file also came with a metadata.csv file which had the ids and labels of the associated 'videos'. Each data batch came in a zip file in the naming format of 'deepfake-detection-faces-part-i-j.zip' where 'i-j' refers to the batch number. I created a bash script *unzip_batch.sh* to unzip a zip file given a batch number and appropriately rename its associated metadata file with 'metadata_i-j.csv'. All metadata were consolidated into a single metadata file with this simple bash command:  

`cat data/metadata*.csv > data/metadata.csv`

## Data Cleaning
---

### Filtering Based on Number of Frames  

Based on the original dataset comprising of videos that are all 10 seconds long at 30 FPS, I should expect the pre-processed dataset to consist of 300 images or frames of faces for each video. From browsing the pre-processed data, I encountered cases where frames misidentified as faces were present and cases with more than 1 face per frame (video featured more than 1 actor). I thus investigated the number of frames across the dataset. Video filenames and their number of frames was obtained using a bash script [**get_n_frames.sh**](https://github.com/sdlee94/BrainStation_Capstone/blob/master/get_n_frames.sh) and written into a csv file (**n_frames.csv**).

<p align="center">
  <img src="https://github.com/sdlee94/BrainStation_Capstone/tree/master/figs/n_frames_hist.png"/>
  <br/>
</p>  

> We can see that the majority of the dataset have around 300 frames, as expected. There is also a peak around 600 frames which comprise of videos with 2 actors. I decided to just keep the videos with between 200-400 frames since most of my data fall within this range. This is to help eliminate outliers and simplify things later when I extract frames from each video.  

## Building Deep Learning Models for DeepFake detection
---


### Detection using a custom CNN with 1 Frame per Video

### Detection using Transfer Learning with 1 Frame per Video

### Detection using Time Distributed CNN + Recurrent NN with 30 Frames per video

## Project Summary
---
