# My BrainStation Capstone Project: Deep Learning for DeepFake Detection
---

### What are DeepFakes?

Take a look at this GIF. Which one do you think is the original clip?  

<p align="center">
  <img src="https://github.com/sdlee94/BrainStation_Capstone/blob/master/deepfake.gif"/>
  <br/>
  <a href="https://tenor.com/view/deep-fakes-ctrl-shift-face-smiles-shrugs-gif-14641388">Source</a>
</p>  

Difficult, right? Especially if you don't know what movie this is from. Did you know this was generated by a computer? Well if you aren't aware, this is a **DeepFake**, an altered video produced by **Artificial Intelligence (AI)**. And it's not just limited to face swapping, any aspect of digital media is subject to 'DeepFaking' - a person's mouth movements can be adjusted to match any audio sample, for instance. It also takes a fraction of the time and cost to make DeepFakes than if a person were to produce the same results with CGI. This technology brings major implications to society; from spreading false information to fabricating evidence in court to sabotaging politics, it's not hard to imagine the many dangers of DeepFakes if left unchecked. Imagine if DeepFakes were to come out today of health officials and political figures saying that the Covid-19 pandemic is over, everyone can go out and socialize - the consequences would be disastrous!

### DeepFake Detection Challenge  

In an effort to curb the emerging threat of DeepFakes, a [**Kaggle competition**](https://www.kaggle.com/c/deepfake-detection-challenge/overview) was built in collaboration between Amazon, Facebook, Microsoft and Partnership on AI to invite enthusiasts of all backgrounds to compete for the best performing DeepFake detection model. As someone who loves challenging problems and has genuine concerns about DeepFakes, I chose to tackle this challenge for my capstone project. Over the course of ~7 weeks I performed an end-to-end Data Science workflow in which I obtained data, processed it and trained several deep learning models to differentiate between real and fake videos. To date, my best model achieved **96% precision** (of all predicted fakes, 96% were indeed fake) and **83% specificity** (able to correctly identify 83% of real videos) using just a single frame per video. Below details the tools used for this project and an overview of the steps I took to get there!

## Resources
---

### Tools Used
- Bash
- Python
- Github
- Google Colab

#### Python Packages:
- **Data Science**: numpy, pandas
- **Plotting**: matplotlib
- **Machine Learning**: tensorflow version 2.1, keras
- **Other**: jupyter, os, imageio, pickle, h5py

### Dataset

The competition dataset consisted of close to 500 GB of videos, each with a length of 10 seconds at 30 frames per second (FPS). Due to computational and time limitations, I opted to use a pre-processed dataset which consisted of 160x160 resolution images of extracted faces from the original videos. Credits goes to *Hieu Phung* for generating this [**dataset**](https://www.kaggle.com/c/deepfake-detection-challenge/discussion/128954) - information about the pre-processing workflow can be found [**here**](https://www.kaggle.com/phunghieu/deepfake-detection-face-extractor). I also downloaded a subsample of the entire dataset, consisting of 10,420 videos (extracted frames).  

The dataset was split into several parts, with a zip file containing a variable number of folders with each containing a set of images pertaining to a unique video. Each zip file also came with a metadata.csv file which had the ids and labels of the associated 'videos'. Each data batch came in a zip file in the naming format of 'deepfake-detection-faces-part-i-j.zip' where 'i-j' refers to the batch number. I created a bash script *unzip_batch.sh* to unzip a zip file given a batch number and appropriately rename its associated metadata file with 'metadata_i-j.csv'. All metadata were consolidated into a single metadata file with this simple bash command:  

`cat data/metadata*.csv > data/metadata.csv`

Important aspects of this dataset (relevance explained later):
- Imbalanced classes: ~88% labeled **Fake**, ~12% labeled **Real**
- Multiple fakes derived from each original

## Exploratory Data Analysis and Data Cleaning
---

### Filtering Based on Number of Frames  

Based on the original dataset comprising of videos that are all 10 seconds long at 30 FPS, I should expect the pre-processed dataset to consist of 300 images or frames of faces for each video. From browsing the pre-processed data, I encountered cases where frames misidentified as faces were present and cases with more than 1 face per frame (video featured more than 1 actor). I thus investigated the number of frames across the dataset. Video filenames and their number of frames was obtained using a bash script [**get_n_frames.sh**](https://github.com/sdlee94/BrainStation_Capstone/blob/master/get_n_frames.sh) and written into a csv file (**n_frames.csv**). The distribution of frame numbers was then plotted using Seaborn (see [notebook](https://github.com/sdlee94/BrainStation_Capstone/blob/master/Data%20Cleaning.ipynb))

<p align="center">
  <img src="https://github.com/sdlee94/BrainStation_Capstone/blob/master/figs/n_frames_hist.png"/>
  <br/>
</p>  

> There is some variability in the number of frames that were extracted from each video, with the majority of extracted frames falling between 200 and 400. There is also a noticeable group around 600 frames - these must be the videos that featured 2 actors.  

Some explanations on why some videos had differing frame numbers are as follows: the ones with more frames had extra ones due to misidentified faces during pre-processing while the ones with less frames may had gaps in the video in which the actor's face was not detectable (e.g. actor may have turned their head or moved out of view) These factors can pose problems during classification since these extra frames or gaps would result in an inconsistent sequence of images. So I removed these 'outliers' and kept just the videos with between 200 and 400 frames since most of my data were within this range (see [notebook](https://github.com/sdlee94/BrainStation_Capstone/blob/master/Data%20Cleaning.ipynb)). Outlier names were exported as `n_frame_outliers.txt` and then used to move the matching directories into an archive folder:

`xargs -a n_frame_outliers.txt mv -t data/archived/n_frame_outliers`

> If the above returns `mv: cannot stat '<path>'$'\r': No such file or directory`, run `tr -d '\r' <n_frame_outliers.txt >n_frame_outliers_new.txt && mv n_frame_outliers_new.txt n_frame_outliers.txt`

After this filtering steps, 8,537 videos remained in my dataset.

### Extracting 30 Frames per video

Since I used keras models which takes a non-variable input shape, I needed all of my videos to have the same number of frames. I also considered whether 300 frames per second is necessary, as many frames will be nearly identical. With my limited resources in time and computation, I rationalized that reducing down to 3 Frames per second (30 frames per video) was a reasonable idea.

## Building Deep Learning Models for DeepFake detection
---


### Detection using a custom CNN with 1 Frame per Video

### Detection using Transfer Learning with 1 Frame per Video

### Detection using Time Distributed CNN + Recurrent NN with 30 Frames per video

## Project Summary
---
